{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '/Users/nehiljain/code/find-your-mate-ai/src' not in sys.path:\n",
    "    sys.path.append('/Users/nehiljain/code/find-your-mate-ai/src')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from find_your_mate_ai.config import settings\n",
    "from find_your_mate_ai.data_ingestion import *\n",
    "\n",
    "nodes = load_nodes_from_mongodb(settings.MONGO_URI)\n",
    "nodes = nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from find_your_mate_ai.data_ingestion import *\n",
    "qa_df = generate_synthetic_questions_data(nodes, \"/tmp\")\n",
    "qa_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "# llm = Ollama(model=\"llama3\")\n",
    "llm = OpenAI()\n",
    "openai.api_key = settings.OPENAI_API_KEY\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"Who are some candidates who have worked at facebook, meta, google or linkedin?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.metadata['file_name'])\n",
    "    display_source_node(node, source_length=1000)\n",
    "    print(\"-\"*100)\n",
    "    print(\"-\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].text, nodes[0].doc_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz\n",
    "import uuid\n",
    "\n",
    "# Define the path to the labeled data from Label Studio\n",
    "labeled_data_path = '/Users/nehiljain/Downloads/project-1-at-2024-05-10-23-13-9a0696b7.json'\n",
    "\n",
    "# Load the labeled data into a DataFrame\n",
    "df = pd.read_json(labeled_data_path)\n",
    "\n",
    "# Function to extract curated questions from the labeled data\n",
    "def extract_curated_questions(row):\n",
    "    # Access the 'annotations' field from each row\n",
    "    curated_questions = [\n",
    "        text\n",
    "        for item in row['annotations']\n",
    "        for value in item['result']\n",
    "        if value['from_name'] == 'question'  # Filter for items labeled as 'question'\n",
    "        for text in (value['value']['text'] if isinstance(value['value']['text'], list) else [value['value']['text']])\n",
    "    ]\n",
    "    # Store the extracted questions back into the 'data' field under 'curated_questions'\n",
    "    row['data']['curated_questions'] = curated_questions\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "df = df.apply(extract_curated_questions, axis=1)\n",
    "\n",
    "# Create a list of dictionaries containing the question and context for each item\n",
    "curated_data = [\n",
    "    {'question': question, 'context': context}\n",
    "    for item in df['data']\n",
    "    for question in item['curated_questions']\n",
    "    for context in item['answer_contents']\n",
    "]\n",
    "\n",
    "# Create a dictionary to map context to doc_id using fuzzy matching\n",
    "context_to_doc_id = {}\n",
    "for node in nodes:\n",
    "    context_to_doc_id[node.text] = node.doc_id\n",
    "\n",
    "# Function to find the best matching doc_id for a given context\n",
    "def find_best_matching_doc_id(context, context_to_doc_id):\n",
    "    best_match = None\n",
    "    highest_ratio = 0\n",
    "    for text, doc_id in context_to_doc_id.items():\n",
    "        ratio = fuzz.partial_ratio(context, text)\n",
    "        if ratio > highest_ratio:\n",
    "            highest_ratio = ratio\n",
    "            best_match = doc_id\n",
    "        if highest_ratio > 95:  # Break early if a very high match is found\n",
    "            break\n",
    "    return best_match\n",
    "\n",
    "# Update curated_data with 'context_id' by finding the best matching doc_id\n",
    "for item in curated_data:\n",
    "    context = item['context']\n",
    "    best_matching_doc_id = find_best_matching_doc_id(context, context_to_doc_id)\n",
    "    item['context_id'] = best_matching_doc_id\n",
    "\n",
    "# Prepare data structures for the EmbeddingQAFinetuneDataset\n",
    "queries = {}\n",
    "corpus = {}\n",
    "relevant_docs = {}\n",
    "\n",
    "# Populate the data structures with the curated data\n",
    "for item in curated_data[:15]:\n",
    "    query_id = uuid.uuid4().hex\n",
    "    queries[query_id] = item['question']\n",
    "    doc_id = item['context_id']\n",
    "    corpus[doc_id] = item['context']\n",
    "    if query_id not in relevant_docs:\n",
    "        relevant_docs[query_id] = []\n",
    "    relevant_docs[query_id].append(doc_id)\n",
    "\n",
    "# Create an instance of EmbeddingQAFinetuneDataset with the prepared data\n",
    "embedding_qa_dataset = EmbeddingQAFinetuneDataset(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs\n",
    ")\n",
    "\n",
    "# Display the dataset\n",
    "embedding_qa_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "# llama3 = Ollama(model=\"llama3\")\n",
    "llm = OpenAI(\"gpt-4\")\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "output_path = Path(\"/tmp/qa_dataset.json\")\n",
    "qa_dataset.save_json(output_path)\n",
    "qa_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"pg_eval_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_cohere_rerank = False\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from find_your_mate_ai.config import settings\n",
    "import os\n",
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "\n",
    "if include_cohere_rerank:\n",
    "    metrics.append(\n",
    "        \"cohere_rerank_relevancy\"  # requires COHERE_API_KEY environment variable to be set\n",
    "    )\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_result = await retriever_evaluator.aevaluate_dataset(embedding_qa_dataset)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out on an entire dataset\n",
    "# eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    columns = {\"retrievers\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(\"top-2 eval\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "# Correcting the lambda function to properly access attributes of the RetrievalEvalResult object\n",
    "sorted_eval_result = sorted(eval_result, key=lambda x: (x.metric_vals_dict['mrr'], -x.metric_vals_dict['hit_rate']))\n",
    "eval_result_item = sorted_eval_result[1]\n",
    "wrapped_query = textwrap.fill(eval_result_item.query, width=80)\n",
    "pprint(eval_result_item.dict())\n",
    "# print(\"-\"*100)\n",
    "# print(wrapped_query)\n",
    "# print(\"-\"*100)\n",
    "# print(textwrap.fill(eval_result_item.retrieved_texts[-1], width=80))\n",
    "# # Sorting eval_result based on MRR in ascending order and then HIT Rate in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
